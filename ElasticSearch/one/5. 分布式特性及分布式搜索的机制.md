## 集群分布式模型及选主与脑裂问题

### 分布式特性

- Elasticsearch的分布式架构带来的好处
  - 存储的水平扩容，支持PB级数据
  - 提高系统的可用性，部分节点停止服务，整个集群的服务不受影响
- Elasticsearch的分布式架构
  - 不同的集群通过不同的名字来区分，默认名字“elasticsearch”
  - 通过配置文件修改，或者在命令行中 -E cluster.name=geektime进行设定

### 节点

- 节点是一个Elasticsearch的实例
  - 其本质上就是一个JAVA进程
  - 一台机器上可以运行多个Elasticsearch进程，但是生产环境一般建议一台机器上就运行一个Elasticsearch实例
- 每一个节点都有名字，通过配置文件配置，或者启动时候-E node.name=geektime指定
- 每一个节点在启动之后，会分配一个UID，保存在data目录下

### **Coordinating Node**

- 处理请求的节点，叫Coordinating Node
  - 路由请求到正确的节点，例如创建索引的请求，需要路由到Master节点
- 所有节点默认都是Coordinating Node
- 通过将其他类型设置为False，使其成为Dedicated Coordinating Node

Demo - 启动节点，Cerebro介绍

- 启动一个节点的
  - bin/elasticsearch -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data -E http.port=9200
- https://github.com/lmenezes/cerebro/releases
  - Overview /Filter by node/index
  - Nodes
  - REST / More
  - Health Status

### Data Node

- 可以保存数据的节点，叫做Data Node
  - 节点启动后，默认就是数据节点。可以设置node.data:false禁止
- Data Node的职责
  - 保存分片数据。在数据扩展上起到了至关重要的作用（由Master Node决定如何把分片分发到数据节点上）
- 通过增加数据节点
  - 可以解决数据水平扩展和解决数据单点的问题

```shell
bin/elasticsearch -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data -E http.port=9201
```

### Master Node

- Master Node的职责
  - 处理创建，删除索引等请求/决定分片被分配到哪个节点/负责索引的创建与删除
  - 维护并且更新Cluster State
- Master Node的最佳实践
  - Master节点非常重要，在部署上需要考虑解决单点的问题
  - 为了一个集群设置多个Master节点/每个节点只能承担Master的单一角色

### Master Eligible Nodes & 选主的过程

- 互相Ping对方，Node id低的会成为被选举的节点
- 其他节点会加入集群，但是不承担Master节点的角色。一旦发现被选中的主节点丢失，就会选举出新的Master节点

### 脑裂问题

- Split - Brain，分布式系统的经典网络问题，当出现网络问题，一个节点和其他节点无法连接
  - Node 2和Node 3会重新选举Master
  - Node 1自己还是作为Master，组成一个集群，同时更新Cluster State
  - 导致2个master，维护不同的cluster state。当网络恢复时，无法选择正确恢复

**如何避免脑裂问题**

- 限定一个选举条件，设置quorum（仲裁），只有在Master eligible节点数大于quorum时，才能进行选举
  - Quorum = （master节点总数/2）+ 1
  - 当3个master eligible时，设置discovery.zen.minimum_master_nodes为2，即可避免脑裂
- 从7.0开始，无需这个配置
  - 移除minimum_master_nodes参数，让Elasticsearch自己选择可以形成仲裁的节点
  - 典型的主节点选举现在只需要很短的时间就可以完成。集群的伸缩变得更安全、更容易，并且可能造成丢失数据的系统配置选项更少了
  - 节点更清楚地记录它们的状态，有助于诊断为什么它们不能加入集群或为什么无法选举出主节点



## 分片与集群的故障转移

### Primary Shard - 提升系统存储容量

- 分片是Elasticsearch分布式存储的基石
  - 主分片/副本分片
- 通过主分片，将数据分布在所有的节点上
  - Primary Shard，可以将一份索引的数据，分散在多个Data Node上，实现存储的水平扩展
  - 主分片（Primary Shard）数在索引创建时候指定，后续默认不能修改，如需修改，需要重建索引

### Replica Shard - 提供数据可用性

- 数据可用性
  - 通过引入副本分片（Replica Shard）提高数据的可用性。一旦主分片丢失，副本分片可以Promote成主分片。副本分片数可以动态调整。每个节点上都有完备的数据。如果不设置副本分片，一旦出现节点硬件故障，就有可能造成数据丢失
- 提升系统的读取性能
  - 副本分片由主分片（Primary Shard）同步。通过支持增加Replica个数，一定程度可以提高读取的吞吐量

### 分片数的设定

- 如何规划一个索引的主分片数和副本分片数
  - 主分片数过小：例如创建了1个Primary Shard的Index
    - 如果该索引增长很快，集群无法通过增加节点实现对这个索引的数据扩展
  - 主分片数设置过大：导致单个Shard容量很小，引发一个节点上过多分片，影响性能
  - 副本分片数设置过多，会降低集群整体的写入性能 

**单节点集群**

- 副本无法分片，集群状态黄色

```json
PUT tmdb
{
    "settings":{
        "number_of_shards":3,
        "number_of_replicas":1
    }
}
```

**增加一个数据节点**

- 集群状态转为绿色
- 集群具备故障转移能力
- 尝试着将Replica设置为2和3，查看集群的状况

**再增加一个数据节点**

- 集群具备故障转移能力
- Master节点决定分片分配到哪个节点
- 通过增加节点，提高集群的计算能力

**故障转移**

- 3个节点共同组成。包含了1个索引，索引设置了3个Primary Shard和1个Replica
- 节点1是Master节点，节点意外出现故障。集群重新选举Master节点
- Node 3上的RO提升成PO，集群变黄
- R0和R1分配，集群变绿

**集群健康状态**

- Green：健康状态，所有的主分片和副本分片都可用
- Yellow：亚健康，所有主分片可用，部分副本分片不可用
- Red：不健康状态，部分主分片不可用

```json
GET /_cluster/health
```



## 文档分布式存储

### 文档存储在分片上

- 文档会存储在具体的某个主分片和副本分片上：例如 文档 1，会存储在P0 和 R0分片上
- 文档到分片的映射算法
  - 确保文档能均匀分布在所用的分片上，充分利用硬件资源，避免部分机器空闲，部分机器繁忙
  - 潜在算法
    - 随机/Round Robin。当查询文档1，分片数很多，需要多次查询才可能查到文档1
    - 维护文档到分片的映射关系，当文档数据量大的时候，维护成本高
    - 实时计算，通过文档1，自动算出，需要去那个分片上获取文档

### 文档到分片的路由算法

- shard = hash（_routing）% number_of_primary_shards
  - Hash算法确保文档均匀分散到分片中
  - 默认的_routing值是文档id
  - 可以自行制定routing数值，例如用相同国家的商品，都分配到指定的shard
  - 设置Index Settings后，Primary数，不能随意修改的根本原因

## 分片及其生命周期

### 分片的内部原理

- 什么是ES的分片
  - ES中最小的工作单元/是一个Lucene的Index
- 一些问题
  - 为什么ES的搜索是近实时的（1秒后被搜索到）
  - ES如何保证在断电时数据也不会丢失
  - 为什么删除文档，并不会立刻释放空间

### 倒排索引不可变性

- 倒排索引采用Immutable Design，一旦生成，不可改变
- 不可变性，带来的好处如下：
  - 无需考虑并发写文件的问题，避免了锁机制带来的性能问题
  - 一旦读入内核的文件系统缓存，便留在哪里。只要文件系统存有足够的空间，大部分请求就会直接请求内存，不会命中磁盘，提升了很大的性能
  - 缓存容易生成和维护 / 数据可以被压缩
- 不可变更性，带来的挑战性：如果需要让一个新的文档可以被搜索，需要重新建整个索引。

### Lucene Index

-  在Lucene中，单个倒排索引文件被称为Segment。Segment是自包含的，不可变更的。多个Segments汇总在一起，称为Lucene的Index，其对应的就是ES中的Shard
- 当有新文档写入时，会生成新Segment，查询时会同时查询所有Segments，并且对结果汇总。Lucene中有一个文件，用来记录所有Segments信息，叫做Commit Point
- 删除的文档信息，保存在“.del”文件中

### 什么是Refresh

- 将Index buffer写入Segment的过程叫Refresh。Refresh不执行fsync操作
- Refresh频率L默认1秒发生一次，可通过Index.refresh_interval配置。Refresh后，数据就可以被搜索到了。这也是为什么Elasticsearch被称为近实时搜索
- 如果系统有大量的数据写入，那就会产生很多Segment
- Index Buffer被占满时，会触发Refresh，默认值是JVM的10%

### 什么是Transaction Log

- Segment写入磁盘的过程相对耗时，借助文件系统缓存，Refresh时，先将Segment写入缓存以开放查询
- 为了保证数据不会丢失。所以在Index文档时，同时写Transcation Log，高版本开始，Transcation Log默认落盘。每个分片有一个Transaction Log
- 在ES Refresh时，Index Buffer被清空，Transaction log不会清空